{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunyeul/ToyProjectLab/blob/feature%2Fnanogpt_tutorial/nanoGPT/nanoGPT_tutorial_char_enc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ojEXoQ4BNv"
      },
      "source": [
        "# nanoGPTのコードレビュー\n",
        "\n",
        "- https://github.com/karpathy/nanoGPT\n",
        "\n",
        "![](https://github.com/karpathy/nanoGPT/raw/master/assets/nanogpt.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjKSVyABIh1r",
        "outputId": "a6151154-6230-4447-fe64-ce0dffc8c595"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mon Apr 24 17:40:48 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   31C    P0    42W / 400W |      0MiB / 40960MiB |      0%      Default |\n",
            "|                               |                      |             Disabled |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bodtpQhz4Ia7"
      },
      "source": [
        "## 事前設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb62e3eS4NT_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "torch.manual_seed(3655)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    vocab_size: int = 65 # 50_304\n",
        "    batch_size: int = 64\n",
        "    block_size: int = 256 # what is the maximum context length for predictions?\n",
        "\n",
        "    train_size: float = 0.8  # valid_sizeは自動で0.2に決まる\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int= 384\n",
        "    dropout: float = 0.2\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3TvpII24Eug"
      },
      "source": [
        "## データのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OQA6_SNxlY-",
        "outputId": "59f4ffef-7912-4c99-f047-8498783a6d6a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-04-24 17:40:58--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.02s   \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-04-24 17:40:59 (59.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# データセットをダウンロードしましょう。\n",
        "!wget -N https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SAFFbOYxlZC",
        "outputId": "3b856322-7db4-492f-d3a2-4f10d722fe15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(len(text))\n",
        "print(text[:1_000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkJ-bUzPxlZE",
        "outputId": "f66404b4-d97d-48e8-e5f3-f129dc8d1403"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# テキストから重複を除いた文字列を取得し、アルファベット順にソートする\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"\".join(chars))\n",
        "\n",
        "# ボキャブラリーのサイズを取得する\n",
        "vocab_size = len(chars)\n",
        "Config.vocab_size = vocab_size\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVRHSYZK0f6_"
      },
      "outputs": [],
      "source": [
        "# 文字列をインデックスに変換するための辞書を作成する\n",
        "# s2iは文字列をインデックスに変換するための辞書\n",
        "s2i = {ch:i for i, ch in enumerate(chars)}\n",
        "\n",
        "# インデックスを文字列に変換するための辞書を作成する\n",
        "# i2sはインデックスを文字列に変換するための辞書\n",
        "i2s = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# 文字列を数値のリストに変換する関数を定義する\n",
        "encode = lambda s: [s2i[c] for c in s]\n",
        "\n",
        "# 数値のリストを文字列に変換する関数を定義する\n",
        "decode = lambda l: ''.join([i2s[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbvQrxBb0s5g",
        "outputId": "cb6d4d8d-5e4f-4aff-fc58-c8bb1df6f853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ],
      "source": [
        "# テキストを数値のリストに変換する\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "\n",
        "# データの形状とデータ型を表示する\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# 先頭の1000文字を表示する\n",
        "print(data[:1000]) # GPTにとっては、ここで表示される1000文字は以下のようになる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gE3jEpI4Ts4"
      },
      "source": [
        "## train/validスプリット"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPakLTBJxlZF"
      },
      "outputs": [],
      "source": [
        "# 学習用データと検証用データに分割する\n",
        "n = int(Config.train_size * len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_WgbEdw15o1",
        "outputId": "a2e8fe93-9bde-47b9-ba4a-744a19f0af6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "入力がtensor([18])のときに、正解は47です\n",
            "入力がtensor([18, 47])のときに、正解は56です\n",
            "入力がtensor([18, 47, 56])のときに、正解は57です\n",
            "入力がtensor([18, 47, 56, 57])のときに、正解は58です\n",
            "入力がtensor([18, 47, 56, 57, 58])のときに、正解は1です\n",
            "入力がtensor([18, 47, 56, 57, 58,  1])のときに、正解は15です\n",
            "入力がtensor([18, 47, 56, 57, 58,  1, 15])のときに、正解は47です\n",
            "入力がtensor([18, 47, 56, 57, 58,  1, 15, 47])のときに、正解は58です\n",
            "入力がtensor([18, 47, 56, 57, 58,  1, 15, 47, 58])のときに、正解は47です\n",
            "入力がtensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47])のときに、正解は64です\n"
          ]
        }
      ],
      "source": [
        "# 学習用データを最初のConfig.block_size文字だけに限定する\n",
        "x = train_data[:Config.block_size]\n",
        "\n",
        "# 学習用データを2文字目から最初のConfig.block_size+1文字に限定する\n",
        "y = train_data[1:Config.block_size+1]\n",
        "\n",
        "# 10回繰り返す\n",
        "for t in range(10):\n",
        "\n",
        "    # 入力となる文字列を1文字からt+1文字までに限定する\n",
        "    context = x[:t+1]\n",
        "\n",
        "    # 正解の文字を取得する\n",
        "    target = y[t]\n",
        "\n",
        "    # 入力がcontextのときに、正解がtargetであることを表示する\n",
        "    print(f\"入力が{context}のときに、正解は{target}です\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bcrwukc15mf"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    # 入力と正解の小さなバッチを生成する\n",
        "    # splitが'train'の場合は学習用データから、'val'の場合は検証用データからデータを取得する\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    \n",
        "    # バッチを開始するためのランダムなインデックスを生成する\n",
        "    idx = torch.randint(high=len(data) - Config.block_size, size=(Config.batch_size,))\n",
        "    \n",
        "    # xは、block_sizeの長さのシーケンスのバッチである\n",
        "    x = torch.stack([data[i:i+Config.block_size] for i in idx])  # [batch_size, block_size]\n",
        "    \n",
        "    # yは、xと同じものであるが、1つずつずれている\n",
        "    y = torch.stack([data[i+1:i+Config.block_size+1] for i in idx])  # [batch_size, block_size]\n",
        "    \n",
        "    x, y = x.to(Config.device), y.to(Config.device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3QdM1D315hN",
        "outputId": "31ac03b5-73a3-4ca4-b4f7-5a616124c0f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([64, 256])\n",
            "tensor([[43,  1, 58,  ..., 50, 63,  6],\n",
            "        [50,  1, 58,  ..., 43,  1, 41],\n",
            "        [57,  1, 42,  ..., 47, 41, 46],\n",
            "        ...,\n",
            "        [46, 43, 57,  ...,  1, 57, 53],\n",
            "        [56,  1, 46,  ...,  1, 58, 46],\n",
            "        [ 0, 27, 56,  ..., 32, 10,  0]], device='cuda:0')\n",
            "targets:\n",
            "torch.Size([64, 256])\n",
            "tensor([[ 1, 58, 46,  ..., 63,  6,  1],\n",
            "        [ 1, 58, 46,  ...,  1, 41, 39],\n",
            "        [ 1, 42, 47,  ..., 41, 46, 39],\n",
            "        ...,\n",
            "        [43, 57, 58,  ..., 57, 53,  1],\n",
            "        [ 1, 46, 43,  ..., 58, 46, 43],\n",
            "        [27, 56,  1,  ..., 10,  0, 20]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD6QvHMA4bTA"
      },
      "source": [
        "## nanoGPTモデルの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNJkesvGF6ma"
      },
      "outputs": [],
      "source": [
        "def new_gelu(x):\n",
        "    \"\"\"\n",
        "    Google BERT repoに現在実装されているGELU活性化関数の実装（OpenAI GPTと同じ）。\n",
        "    参考文献：Gaussian Error Linear Units（GELU）論文：https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" オプションのバイアスを持つLayerNorm。PyTorchではLayerNormでbias=Falseを簡単にサポートしていない。 \"\"\"\n",
        "    \n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        # 正規化の重みパラメータ\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        # オプションのバイアス\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "        \n",
        "    def forward(self, input):\n",
        "        # Layer Normalizationを計算する\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, eps=1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # 全てのヘッドに対するkey、query、valueのプロジェクションをバッチ内で実行\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # 出力のプロジェクション\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # 正則化\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # Flash AttentionはGPUを加速するが、PyTorch >= 2.0でのみサポートされる\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # 入力シーケンスの左側にのみ注意が適用されることを保証する因果マスク\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # バッチサイズ、シーケンス長、埋め込みサイズ（n_embd）\n",
        "\n",
        "        # バッチ全体の全てのヘッドのquery、key、valuesを計算し、ヘッドをバッチの先頭に移動\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # [B, n_head, T, head_size]\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # [B, n_head, T, head_size]\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # [B, n_head, T, head_size]\n",
        "        \n",
        "        # 因果的自己注意機構; self-attend: [B, n_head, T, head_size] x [B, n_head, head_size, T] -> [B, n_head, T, T]\n",
        "        if self.flash:\n",
        "            # Flash Attention CUDAカーネルを使用した効率的な注意\n",
        "            y = nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
        "        else:\n",
        "            # 注意の手動実装\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v  # [B, n_head, T, T] x [B, n_head, T, head_size] -> [B, n_head, T, head_size]\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # すべてのヘッドの出力を横並びに再構築\n",
        "        \n",
        "        # 出力のプロジェクション\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # 入力を4倍に拡張する線形層\n",
        "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4, bias=config.bias)\n",
        "        # 入力サイズに戻す線形層\n",
        "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd, bias=config.bias)\n",
        "        # GELU非線形関数\n",
        "        self.act = new_gelu\n",
        "        # ドロップアウト\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # GELU非線形関数を適用する\n",
        "        h = self.act(self.c_fc(x))\n",
        "        # 元のサイズに戻す\n",
        "        h2 = self.c_proj(h)\n",
        "        # 出力をドロップアウト\n",
        "        return self.dropout(h2)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Causal Self-Attention\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        # Layer Normalization\n",
        "        self.ln_1 = LayerNorm(config.n_embd, config.bias)\n",
        "        # MLP\n",
        "        self.mlp = MLP(config)\n",
        "        # Layer Normalization\n",
        "        self.ln_2 = LayerNorm(config.n_embd, config.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Multi-Head Attentionを適用する\n",
        "        x = x + self.attn(self.ln_1(x))  # 元のコードはこちら\n",
        "        # x = x + self.ln_1(self.attn(x))\n",
        "        # MLPを適用する\n",
        "        x = x + self.mlp(self.ln_2(x))  # 元のコードはこちら\n",
        "        # x = x + self.ln_2(self.mlp(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "    \n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, config.bias)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)    \n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of lengh {t}, block size is {self.config.block_size}\"\n",
        "        \n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # shape [1, t]\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape [b, t, n_embd]\n",
        "        pos_emb = self.transformer.wpe(pos)  # positional embeddings of shape [1, t, n_embd]\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)  # [b, t, n_embd]\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)  # [b, t, n_embd]\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x) # [b, t, vocab_size]\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :])  # [b, 1, vocab_size]  note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "        \n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequencecontext is growing too long, we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)  # [b, min(idx_cond.size(1), block_size), vocab_size]\n",
        "            # pluck the logits at the final step and scale by desired terperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution to get the next index\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat([idx, idx_next], dim=1)\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGwTtXeV4hIJ"
      },
      "source": [
        "## 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffqz6oiSSlWh",
        "outputId": "bab810b9-90fb-4214-e061-384e2504d691"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(65, 384)\n",
              "    (wpe): Embedding(256, 384)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
              "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_1): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=65, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# モデルをインスタンス化する\n",
        "model = GPT(config=Config)\n",
        "model.to(device=Config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FjnmsdVZpzP",
        "outputId": "dd357d15-d945-4623-8076-71bfe6ea7b44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "51777\n"
          ]
        }
      ],
      "source": [
        "# 学習可能なパラメーター数を計算\n",
        "num_params = 0\n",
        "\n",
        "for parameter in model.parameters():\n",
        "    num_params += len(parameter)\n",
        "\n",
        "print(num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-I32pOoxlZG",
        "outputId": "8b586ff9-929d-4cdc-c515-c6295bb395f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64, 256, 65])\n",
            "tensor(4.2784, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "\n",
            "X?zFZSSeksx'3 SS\n",
            "NCF'sy?'Bn$LLWrSINosW!hZnF''w&LCjZ!X$HNXVVLlpn!vhKTh;3.'BbMjn3NYM:jRTl;jO-GXh!yxZbc\n"
          ]
        }
      ],
      "source": [
        "# 順伝播を実行し、ログオッズと損失を取得する\n",
        "logits, loss = model(xb, yb)\n",
        "\n",
        "# logitsの形状と損失を表示する\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# モデルによって生成されたテキストを表示する\n",
        "generated_text = model.generate(idx=torch.zeros((1, 1,), dtype=torch.long, device=Config.device), max_new_tokens=100)[0].tolist()\n",
        "decoded_text = decode(generated_text)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "204fcaa2af39494daf6b99db4f7cafd3",
            "422afc75eff9454083853c31a2fcb58f",
            "6a91da2325b04aecb8074ed191dc7de7",
            "22d459ad52a34f2e981c2a34683cd8a3",
            "263ce1a469fe4b2bb87707a2a0d6f364",
            "2ba7dd6d95ce4966a0eee8947bbece78",
            "2df2bfeef09a4a9fb6a702e0715bcde3",
            "0f50c0031a72480ea40a3bc8c32abb56",
            "d131443d9e5d40f9a1214f5a734b7eb8",
            "db8370c776f74ba1868e1340f6a11d2b",
            "d89a6bf2f2e242c4aee2067f9372b047"
          ]
        },
        "id": "Ae3ZhHTvxlZH",
        "outputId": "21662a79-5111-4808-b820-0eae264f8308"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "204fcaa2af39494daf6b99db4f7cafd3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.6047207117080688\n"
          ]
        }
      ],
      "source": [
        "# PyTorchのオプティマイザを作成する\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 10,000回のステップを実行する\n",
        "EPOCH = 10_000\n",
        "for steps in tqdm(range(EPOCH)):\n",
        "    \n",
        "    # データのバッチをサンプリングする\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # 勾配をゼロに設定\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # 損失を評価する\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # 逆伝播を計算する\n",
        "    loss.backward()\n",
        "\n",
        "    # パラメータを更新する\n",
        "    optimizer.step()\n",
        "\n",
        "# 損失を表示する\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN3KoPB4P9Tj"
      },
      "outputs": [],
      "source": [
        "# Additional information\n",
        "PATH = \"model.pth\"\n",
        "\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            }, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZYSGw3qR70j"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6kgRv4O4kT3"
      },
      "source": [
        "## 文章生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W-X7hbpAQ7i8",
        "outputId": "9118471c-4ddd-4e76-826a-74550ea193da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "PAULINA:\n",
            "There is a passing meless, methinks, I am\n",
            "Enforced to confess it. But, methinks,\n",
            "For, I mean to yourself--\n",
            "\n",
            "LEONTES:\n",
            "Stay me lady you,\n",
            "Put sound to the base is dishonour'd of my banishment.\n",
            "I would deep were all king, but, that, if\n",
            "You be envied, and I would not leave\n",
            "The vaultice of making me: indeed, wereful beguns,\n",
            "Of what I was your quarrel doing, I would not king;\n",
            "Thy father had been high committed in you\n",
            "Even too as egg recompense: 'tis his precise\n",
            "By each one to be alight and wee\n"
          ]
        }
      ],
      "source": [
        "# モデルによって生成されたテキストを表示する\n",
        "generated_text = model.generate(idx=torch.zeros((1, 1), dtype=torch.long, device=Config.device), max_new_tokens=500)[0].tolist()\n",
        "\n",
        "decoded_text = decode(generated_text)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KxcdYXSFxlZH",
        "outputId": "0dbbf7eb-f59b-4cfd-d0f2-9a9dab81ccb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, jest thee this,\n",
            "And be the wish'd and twenty before night.\n",
            "\n",
            "STANLEY:\n",
            "So, my lord, at out--\n",
            "\n",
            "KING RICHARD II:\n",
            "The true war from him: here's four out was now,\n",
            "Which, like a bonny spurpose, not love.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Hath his verges, and his words\n",
            "Requite ith the this cause of your suit?\n",
            "\n",
            "STrike Richard:\n",
            "What noble prevail befell'd his fear?--and for it be prothen;\n",
            "Nor never be stong by any cander regreeth:\n",
            "'Tis shame can no leave but till I aim'd thee,\n",
            "But the nothing done, not art a Montague.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# モデルによって生成されたテキストを表示する\n",
        "\n",
        "prompt = \"Hello, \"\n",
        "generated_text = model.generate(idx=torch.tensor([encode(prompt)], device=Config.device), max_new_tokens=500)[0].tolist()\n",
        "\n",
        "decoded_text = decode(generated_text)\n",
        "print(decoded_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f50c0031a72480ea40a3bc8c32abb56": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "204fcaa2af39494daf6b99db4f7cafd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_422afc75eff9454083853c31a2fcb58f",
              "IPY_MODEL_6a91da2325b04aecb8074ed191dc7de7",
              "IPY_MODEL_22d459ad52a34f2e981c2a34683cd8a3"
            ],
            "layout": "IPY_MODEL_263ce1a469fe4b2bb87707a2a0d6f364"
          }
        },
        "22d459ad52a34f2e981c2a34683cd8a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db8370c776f74ba1868e1340f6a11d2b",
            "placeholder": "​",
            "style": "IPY_MODEL_d89a6bf2f2e242c4aee2067f9372b047",
            "value": " 10000/10000 [19:54&lt;00:00,  8.35it/s]"
          }
        },
        "263ce1a469fe4b2bb87707a2a0d6f364": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba7dd6d95ce4966a0eee8947bbece78": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2df2bfeef09a4a9fb6a702e0715bcde3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "422afc75eff9454083853c31a2fcb58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ba7dd6d95ce4966a0eee8947bbece78",
            "placeholder": "​",
            "style": "IPY_MODEL_2df2bfeef09a4a9fb6a702e0715bcde3",
            "value": "100%"
          }
        },
        "6a91da2325b04aecb8074ed191dc7de7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f50c0031a72480ea40a3bc8c32abb56",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d131443d9e5d40f9a1214f5a734b7eb8",
            "value": 10000
          }
        },
        "d131443d9e5d40f9a1214f5a734b7eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d89a6bf2f2e242c4aee2067f9372b047": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "db8370c776f74ba1868e1340f6a11d2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}