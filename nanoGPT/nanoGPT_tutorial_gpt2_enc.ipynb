{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunyeul/ToyProjectLab/blob/feature%2Fnanogpt_tutorial/nanoGPT/nanoGPT_tutorial_gpt2_enc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5ojEXoQ4BNv"
      },
      "source": [
        "# nanoGPTのコードレビュー\n",
        "\n",
        "- https://github.com/karpathy/nanoGPT\n",
        "\n",
        "![](https://github.com/karpathy/nanoGPT/raw/master/assets/nanogpt.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjKSVyABIh1r",
        "outputId": "1f75d71f-b4ad-426f-b138-2ca24c434201"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 25 09:07:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   48C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bodtpQhz4Ia7"
      },
      "source": [
        "## 事前設定"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzWXUBQfYhsg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41f4eb68-8766-4cd1-bcaf-b7c823a97cb8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.7 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m63.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade -qq tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kb62e3eS4NT_"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import tiktoken\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from dataclasses import dataclass\n",
        "\n",
        "torch.manual_seed(3655)\n",
        "\n",
        "@dataclass\n",
        "class Config:\n",
        "    vocab_size: int = 50_304\n",
        "    batch_size: int = 64\n",
        "    block_size: int = 256 # what is the maximum context length for predictions?\n",
        "\n",
        "    train_size: float = 0.8  # valid_sizeは自動で0.2に決まる\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    n_layer: int = 6\n",
        "    n_head: int = 6\n",
        "    n_embd: int= 384\n",
        "    dropout: float = 0.2\n",
        "    bias: bool = True  # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3TvpII24Eug"
      },
      "source": [
        "## データのダウンロード"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OQA6_SNxlY-",
        "outputId": "e79fc89b-98cf-4da3-969f-00146e2a1288"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-04-25 03:02:25--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.005s  \n",
            "\n",
            "Last-modified header missing -- time-stamps turned off.\n",
            "2023-04-25 03:02:27 (224 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# データセットをダウンロードしましょう。\n",
        "!wget -N https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_SAFFbOYxlZC",
        "outputId": "ba67c077-a2d2-400f-9e11-31a95f7f6ef5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1115394\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "print(len(text))\n",
        "print(text[:1_000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XkJ-bUzPxlZE",
        "outputId": "8737ee64-1f6a-495b-ef14-cddff96bfeb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ],
      "source": [
        "# テキストから重複を除いた文字列を取得し、アルファベット順にソートする\n",
        "chars = sorted(list(set(text)))\n",
        "print(\"\".join(chars))\n",
        "\n",
        "# ボキャブラリーのサイズを取得する\n",
        "vocab_size = len(chars)\n",
        "print(vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tVRHSYZK0f6_"
      },
      "outputs": [],
      "source": [
        "# 文字列をインデックスに変換するための辞書を作成する\n",
        "# s2iは文字列をインデックスに変換するための辞書\n",
        "s2i = {ch:i for i, ch in enumerate(chars)}\n",
        "\n",
        "# インデックスを文字列に変換するための辞書を作成する\n",
        "# i2sはインデックスを文字列に変換するための辞書\n",
        "i2s = {i:ch for i, ch in enumerate(chars)}\n",
        "\n",
        "# 文字列を数値のリストに変換する関数を定義する\n",
        "encode = lambda s: [s2i[c] for c in s]\n",
        "\n",
        "# 数値のリストを文字列に変換する関数を定義する\n",
        "decode = lambda l: ''.join([i2s[i] for i in l])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbvQrxBb0s5g",
        "outputId": "ec064c93-422b-4039-9559-3d728642880d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([338025]) torch.int64\n",
            "tensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11,\n",
            "         3285,   502,  2740,    13,   198,   198,  3237,    25,   198,  5248,\n",
            "          461,    11,  2740,    13,   198,   198,  5962, 22307,    25,   198,\n",
            "         1639,   389,   477, 12939,  2138,   284,  4656,   621,   284,  1145,\n",
            "          680,    30,   198,   198,  3237,    25,   198,  4965,  5634,    13,\n",
            "        12939,    13,   198,   198,  5962, 22307,    25,   198,  5962,    11,\n",
            "          345,   760,   327,  1872,   385,  1526, 28599,   318,  4039,  4472,\n",
            "          284,   262,   661,    13,   198,   198,  3237,    25,   198,  1135,\n",
            "          760,   470,    11,   356,   760,   470,    13,   198,   198,  5962,\n",
            "        22307,    25,   198,  5756,   514,  1494,   683,    11,   290,   356,\n",
            "         1183,   423, 11676,   379,   674,   898,  2756,    13,   198,  3792,\n",
            "          470,   257, 15593,    30,   198,   198,  3237,    25,   198,  2949,\n",
            "          517,  3375,   319,   470,    26,  1309,   340,   307,  1760,    25,\n",
            "         1497,    11,  1497,     0,   198,   198, 12211, 22307,    25,   198,\n",
            "         3198,  1573,    11,   922,  4290,    13,   198,   198,  5962, 22307,\n",
            "           25,   198,  1135,   389, 17830,  3595,  4290,    11,   262,  1458,\n",
            "         1173,  1547,   922,    13,   198,  2061,  4934,   969,  5036,   896,\n",
            "          319,   561, 26958,   514,    25,   611,   484,   198, 19188,  7800,\n",
            "          514,   475,   262, 48713,   414,    11,   981,   340,   547,   198,\n",
            "         1929,  4316,   462,    11,   356,  1244,  4724,   484, 22598,   514,\n",
            "        31533,   306,    26,   198,  4360,   484,   892,   356,   389,  1165,\n",
            "        13674,    25,   262, 10904,  1108,   326,   198,  2001, 42267,   514,\n",
            "           11,   262,  2134,   286,   674, 24672,    11,   318,   355,   281,\n",
            "          198, 24807,   284,  1948,   786,   511, 20038,    26,   674,   198,\n",
            "           82, 13712,   590,   318,   257,  4461,   284,   606,  3914,   514,\n",
            "        15827,   428,   351,   198,   454,   279,  7938,    11,   304,   260,\n",
            "          356,  1716,   374,  1124,    25,   329,   262, 11858,   760,   314,\n",
            "          198, 47350,   428,   287, 16460,   329,  8509,    11,   407,   287,\n",
            "        24613,   329, 15827,    13,   198,   198, 12211, 22307,    25,   198,\n",
            "        17353,   345,  5120,  2592,  1028,   327,  1872,   385,  1526, 28599,\n",
            "           30,   198,   198,  3237,    25,   198, 39276,   683,   717,    25,\n",
            "          339,   338,   257,   845,  3290,   284,   262,  2219,  6017,    13,\n",
            "          198,   198, 12211, 22307,    25,   198, 19626,   345,   644,  2594,\n",
            "          339,   468,  1760,   329,   465,  1499,    30,   198,   198,  5962,\n",
            "        22307,    25,   198, 16371,   880,    26,   290,   714,   307,  2695,\n",
            "          284,  1577,   683,   922,   198, 13116,  6285,    11,   475,   326,\n",
            "          339, 13831,  2241,   351,   852,  6613,    13,   198,   198, 12211,\n",
            "        22307,    25,   198,    45,   323,    11,   475,  2740,   407, 17412,\n",
            "          306,    13,   198,   198,  5962, 22307,    25,   198,    40,   910,\n",
            "        12722,   345,    11,   644,   339, 22027,  1760, 20524,    11,   339,\n",
            "          750,   198,   270,   284,   326,   886,    25,   996,  2705,    12,\n",
            "         5936,   979,  5864,  1450,   460,   307,   198, 11299,   284,   910,\n",
            "          340,   373,   329,   465,  1499,   339,   750,   340,   284,   198,\n",
            "        29688,   465,  2802,   290,   284,   307, 11476,  6613,    26,   543,\n",
            "          339,   198,   271,    11,   772, 10597,   262, 20334,   286,   465,\n",
            "        14675,    13,   198,   198, 12211, 22307,    25,   198,  2061,   339,\n",
            "         2314,  1037,   287,   465,  3450,    11,   345,  1848,   257,   198,\n",
            "        28281,   287,   683,    13,   921,  1276,   287,   645,   835,   910,\n",
            "          339,   318, 25746,    83,   516,    13,   198,   198,  5962, 22307,\n",
            "           25,   198,  1532,   314,  1276,   407,    11,   314,   761,   407,\n",
            "          307, 39497,   286, 14227,    26,   198,   258, 22027, 31025,    11,\n",
            "          351, 18201,    11,   284, 15867,   287, 29693,    13,   198,  2061,\n",
            "        34757,   389,   777,    30,   383,   584,  1735,   267,     6,   262,\n",
            "         1748,   198,   271, 17450,    25,  1521,  2652,   356,   778,   803,\n",
            "          994,    30,   284,   262, 13241,     0,   198,   198,  3237,    25,\n",
            "          198, 16773,    11,  1282,    13,   198,   198,  5962, 22307,    25,\n",
            "          198, 18380,     0,   508,  2058,   994,    30,   198,   198, 12211,\n",
            "        22307,    25,   198,    54, 18906,  6065,   268,  3754,  2449, 14602,\n",
            "           64,    26,   530,   326, 22027,  1464,  6151,   198,  1169,   661,\n",
            "           13,   198,   198,  5962, 22307,    25,   198,  1544,   338,   530,\n",
            "         5508,  1576,    25,   561,   477,   262,  1334,   547,   523,     0,\n",
            "          198,   198, 49275,  1677,    40,  2937,    25,   198,  2061,   670,\n",
            "          338,    11,   616,  1499,  3653,    11,   287,  1021,    30,   810,\n",
            "          467,   345,   198,  3152, 19553,   290,  9784,    30,   383,  2300,\n",
            "           30,  2740,    11,   314, 12472,   345,    13,   198,   198,  5962,\n",
            "        22307,    25,   198,  5122,  1597,   318,   407,  6439,   284,   262,\n",
            "        34548,    26,   484,   423,   198, 18108, 16882,  1359,   428, 46327,\n",
            "          644,   356, 14765,   284,   466,    11,   198,  4758,   783,   356,\n",
            "         1183,   905,   705,   368,   287, 23777,    13,  1119,   910,  3595,\n",
            "          198,  6063,   669,   423,  1913, 45576,    25,   484,  2236,   760,\n",
            "          356,   198, 14150,  1913,  5101,  1165,    13,   198,   198, 49275,\n",
            "         1677,    40,  2937,    25,   198,  5195,    11, 18159,    11,   616,\n",
            "          922,  2460,    11,  6164,  5508, 23788,    11,   198,  8743,   345,\n",
            "        23981, 27012,    30,   198,   198,  5962, 22307,    25,   198,  1135,\n",
            "         2314,    11, 15967,    11,   356,   389, 45171,  1541,    13,   198,\n",
            "          198, 49275,  1677,    40,  2937,    25,   198,    40,  1560,   345,\n",
            "           11,  2460,    11,   749, 21803,  1337,   198, 11980,   262,  1458,\n",
            "         1173,  1547,   286,   345,    13,  1114,   534,  3382,    11,   198,\n",
            "         7120,  7195,   287,   428,   390, 11999,    11,   345,   743,   355,\n",
            "          880,   198, 31584,   379,   262,  9538,   351,   534,   336,  3080,\n",
            "          355, 10303,   606,   198, 39276,   262,  7993,  1181,    11,  3025,\n",
            "         1781,   481,   319,   198,   464,   835,   340,  2753,    11, 25407,\n",
            "         3478,  7319,  1090,  1443,   198,  5189,   517,  1913,  2792,   355,\n",
            "         4625,   621,   460,  1683,   198,  4677,   451,   287,   534, 26795,\n",
            "         3681,    13,  1114,   262,   390, 11999,    11,   198,   464, 11858,\n",
            "           11,   407,   262,  1458,  1173,  1547,    11,   787,   340,    11,\n",
            "          290,   198,  7120, 14475,   284,   606,    11,   407,  5101,    11,\n",
            "         1276,  1037,    13,   978,   441,    11,   198,  1639,   389, 18665,\n",
            "          416, 35765,   414,   198,   817,  1555,   810,   517, 32743,   345,\n",
            "           11,   290,   345, 47397,   198,   464,   932,   907,   267,     6,\n",
            "          262,  1181,    11,   508,  1337,   329,   345,   588, 17150,    11,\n",
            "          198,  2215,   345, 17328,   606,   355,  5775,    13,   198,   198,\n",
            "         5962, 22307,    25,   198, 17784,   329,   514,     0,  6407,    11,\n",
            "         5600,     0,  1119,   497,     6,   263, 19951,   329,   514,   198,\n",
            "        25907,    25,  8659,   514,   284,  1145,   680,    11,   290,   511,\n",
            "         3650,    12, 20089,   198,    66,   859,  1150,   351, 13020,    26,\n",
            "          787,  1225, 14137,   329,   514,  1601,    11,   284,   198, 11284,\n",
            "          514, 17496,    26, 14634,  4445,   597, 17950,   462,   719,   198,\n",
            "        27718,  1028,   262,  5527,    11,   290,  2148,   517,   198,    79,\n",
            "          959,  2259, 24895,  4445,    11,   284,  6333,   510,   290, 39300])\n"
          ]
        }
      ],
      "source": [
        "# テキストを数値のリストに変換する\n",
        "data = torch.tensor(enc.encode_ordinary(text), dtype=torch.long)\n",
        "\n",
        "# データの形状とデータ型を表示する\n",
        "print(data.shape, data.dtype)\n",
        "\n",
        "# 先頭の1000文字を表示する\n",
        "print(data[:1000]) # GPTにとっては、ここで表示される1000文字は以下のようになる"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6gE3jEpI4Ts4"
      },
      "source": [
        "## train/validスプリット"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPakLTBJxlZF"
      },
      "outputs": [],
      "source": [
        "# 学習用データと検証用データに分割する\n",
        "n = int(Config.train_size * len(data))\n",
        "\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_WgbEdw15o1",
        "outputId": "7ee7d1fd-4df7-4a4b-be0c-ce844c0bd2c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "入力がtensor([5962])のときに、正解は22307です\n",
            "入力がtensor([ 5962, 22307])のときに、正解は25です\n",
            "入力がtensor([ 5962, 22307,    25])のときに、正解は198です\n",
            "入力がtensor([ 5962, 22307,    25,   198])のときに、正解は8421です\n",
            "入力がtensor([ 5962, 22307,    25,   198,  8421])のときに、正解は356です\n",
            "入力がtensor([ 5962, 22307,    25,   198,  8421,   356])のときに、正解は5120です\n",
            "入力がtensor([ 5962, 22307,    25,   198,  8421,   356,  5120])のときに、正解は597です\n",
            "入力がtensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597])のときに、正解は2252です\n",
            "入力がtensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252])のときに、正解は11です\n",
            "入力がtensor([ 5962, 22307,    25,   198,  8421,   356,  5120,   597,  2252,    11])のときに、正解は3285です\n"
          ]
        }
      ],
      "source": [
        "# 学習用データを最初のConfig.block_size文字だけに限定する\n",
        "x = train_data[:Config.block_size]\n",
        "\n",
        "# 学習用データを2文字目から最初のConfig.block_size+1文字に限定する\n",
        "y = train_data[1:Config.block_size+1]\n",
        "\n",
        "# 10回繰り返す\n",
        "for t in range(10):\n",
        "\n",
        "    # 入力となる文字列を1文字からt+1文字までに限定する\n",
        "    context = x[:t+1]\n",
        "\n",
        "    # 正解の文字を取得する\n",
        "    target = y[t]\n",
        "\n",
        "    # 入力がcontextのときに、正解がtargetであることを表示する\n",
        "    print(f\"入力が{context}のときに、正解は{target}です\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Bcrwukc15mf"
      },
      "outputs": [],
      "source": [
        "def get_batch(split):\n",
        "    # 入力と正解の小さなバッチを生成する\n",
        "    # splitが'train'の場合は学習用データから、'val'の場合は検証用データからデータを取得する\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    \n",
        "    # バッチを開始するためのランダムなインデックスを生成する\n",
        "    idx = torch.randint(high=len(data) - Config.block_size, size=(Config.batch_size,))\n",
        "    \n",
        "    # xは、block_sizeの長さのシーケンスのバッチである\n",
        "    x = torch.stack([data[i:i+Config.block_size] for i in idx])  # [batch_size, block_size]\n",
        "    \n",
        "    # yは、xと同じものであるが、1つずつずれている\n",
        "    y = torch.stack([data[i+1:i+Config.block_size+1] for i in idx])  # [batch_size, block_size]\n",
        "    \n",
        "    x, y = x.to(Config.device), y.to(Config.device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3QdM1D315hN",
        "outputId": "ea34a498-5521-4d22-ace9-6360c5b46bdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([64, 256])\n",
            "tensor([[   12,  2435,    11,  ...,   293,   913, 14009],\n",
            "        [  318,  6004,  1549,  ...,   266,  2326, 14210],\n",
            "        [ 2767,    25,   198,  ...,  2437,   783,     0],\n",
            "        ...,\n",
            "        [  616, 27806,   198,  ...,   198,  2118,   286],\n",
            "        [12472,   262, 11858,  ...,  8920,   407,   428],\n",
            "        [26560,   444,   286,  ...,  1175,  2339,   402]], device='cuda:0')\n",
            "targets:\n",
            "torch.Size([64, 256])\n",
            "tensor([[ 2435,    11,   618,  ...,   913, 14009,    11],\n",
            "        [ 6004,  1549,   517,  ...,  2326, 14210,  4425],\n",
            "        [   25,   198, 34320,  ...,   783,     0,   508],\n",
            "        ...,\n",
            "        [27806,   198,  5211,  ...,  2118,   286,   262],\n",
            "        [  262, 11858,   339,  ...,   407,   428, 42935],\n",
            "        [  444,   286, 28064,  ...,  2339,   402, 12968]], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD6QvHMA4bTA"
      },
      "source": [
        "## nanoGPTモデルの実装"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNJkesvGF6ma"
      },
      "outputs": [],
      "source": [
        "def new_gelu(x):\n",
        "    \"\"\"\n",
        "    Google BERT repoに現在実装されているGELU活性化関数の実装（OpenAI GPTと同じ）。\n",
        "    参考文献：Gaussian Error Linear Units（GELU）論文：https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return 0.5 * x * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (x + 0.044715 * torch.pow(x, 3.0))))\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    \"\"\" オプションのバイアスを持つLayerNorm。PyTorchではLayerNormでbias=Falseを簡単にサポートしていない。 \"\"\"\n",
        "    \n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        # 正規化の重みパラメータ\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        # オプションのバイアス\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "        \n",
        "    def forward(self, input):\n",
        "        # Layer Normalizationを計算する\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, eps=1e-5)\n",
        "\n",
        "\n",
        "class CausalSelfAttention(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # 全てのヘッドに対するkey、query、valueのプロジェクションをバッチ内で実行\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "        # 出力のプロジェクション\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "        # 正則化\n",
        "        self.attn_dropout = nn.Dropout(config.dropout)\n",
        "        self.resid_dropout = nn.Dropout(config.dropout)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.dropout = config.dropout\n",
        "        # Flash AttentionはGPUを加速するが、PyTorch >= 2.0でのみサポートされる\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
        "            # 入力シーケンスの左側にのみ注意が適用されることを保証する因果マスク\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "    \n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()  # バッチサイズ、シーケンス長、埋め込みサイズ（n_embd）\n",
        "\n",
        "        # バッチ全体の全てのヘッドのquery、key、valuesを計算し、ヘッドをバッチの先頭に移動\n",
        "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # [B, n_head, T, head_size]\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # [B, n_head, T, head_size]\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # [B, n_head, T, head_size]\n",
        "        \n",
        "        # 因果的自己注意機構; self-attend: [B, n_head, T, head_size] x [B, n_head, head_size, T] -> [B, n_head, T, T]\n",
        "        if self.flash:\n",
        "            # Flash Attention CUDAカーネルを使用した効率的な注意\n",
        "            y = nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout, is_causal=True)\n",
        "        else:\n",
        "            # 注意の手動実装\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float(\"-inf\"))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.attn_dropout(att)\n",
        "            y = att @ v  # [B, n_head, T, T] x [B, n_head, T, head_size] -> [B, n_head, T, head_size]\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)  # すべてのヘッドの出力を横並びに再構築\n",
        "        \n",
        "        # 出力のプロジェクション\n",
        "        y = self.resid_dropout(self.c_proj(y))\n",
        "        return y\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # 入力を4倍に拡張する線形層\n",
        "        self.c_fc = nn.Linear(config.n_embd, config.n_embd * 4, bias=config.bias)\n",
        "        # 入力サイズに戻す線形層\n",
        "        self.c_proj = nn.Linear(config.n_embd * 4, config.n_embd, bias=config.bias)\n",
        "        # GELU非線形関数\n",
        "        self.act = new_gelu\n",
        "        # ドロップアウト\n",
        "        self.dropout = nn.Dropout(config.dropout)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # GELU非線形関数を適用する\n",
        "        h = self.act(self.c_fc(x))\n",
        "        # 元のサイズに戻す\n",
        "        h2 = self.c_proj(h)\n",
        "        # 出力をドロップアウト\n",
        "        return self.dropout(h2)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        # Causal Self-Attention\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        # Layer Normalization\n",
        "        self.ln_1 = LayerNorm(config.n_embd, config.bias)\n",
        "        # MLP\n",
        "        self.mlp = MLP(config)\n",
        "        # Layer Normalization\n",
        "        self.ln_2 = LayerNorm(config.n_embd, config.bias)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # Multi-Head Attentionを適用する\n",
        "        x = x + self.attn(self.ln_1(x))  # 元のコードはこちら\n",
        "        # x = x + self.ln_1(self.attn(x))\n",
        "        # MLPを適用する\n",
        "        x = x + self.mlp(self.ln_2(x))  # 元のコードはこちら\n",
        "        # x = x + self.ln_2(self.mlp(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPT(nn.Module):\n",
        "    \n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None\n",
        "        assert config.block_size is not None\n",
        "        self.config = config\n",
        "    \n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            drop = nn.Dropout(config.dropout),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, config.bias)\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)    \n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init all weights\n",
        "        self.apply(self._init_weights)\n",
        "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "    \n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size()\n",
        "        assert t <= self.config.block_size, f\"Cannot forward sequence of lengh {t}, block size is {self.config.block_size}\"\n",
        "        \n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0)  # shape [1, t]\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx)  # token embeddings of shape [b, t, n_embd]\n",
        "        pos_emb = self.transformer.wpe(pos)  # positional embeddings of shape [1, t, n_embd]\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)  # [b, t, n_embd]\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)  # [b, t, n_embd]\n",
        "\n",
        "        if targets is not None:\n",
        "            # if we are given some desired targets also calculate the loss\n",
        "            logits = self.lm_head(x) # [b, t, vocab_size]\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
        "            logits = self.lm_head(x[:, [-1], :])  # [b, 1, vocab_size]  note: using list [-1] to preserve the time dim\n",
        "            loss = None\n",
        "        \n",
        "        return logits, loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=0.7, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequencecontext is growing too long, we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)  # [b, min(idx_cond.size(1), block_size), vocab_size]\n",
        "            # pluck the logits at the final step and scale by desired terperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # sample from the distribution to get the next index\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat([idx, idx_next], dim=1)\n",
        "        \n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGwTtXeV4hIJ"
      },
      "source": [
        "## 学習"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ffqz6oiSSlWh",
        "outputId": "0179465d-a1ca-4847-ae53-22b4cdbde0fc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50304, 384)\n",
              "    (wpe): Embedding(256, 384)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-5): 6 x Block(\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=384, out_features=1152, bias=True)\n",
              "          (c_proj): Linear(in_features=384, out_features=384, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_1): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=384, out_features=1536, bias=True)\n",
              "          (c_proj): Linear(in_features=1536, out_features=384, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=384, out_features=50304, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# モデルをインスタンス化する\n",
        "model = GPT(config=Config)\n",
        "model.to(device=Config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FjnmsdVZpzP",
        "outputId": "d69cbd18-48f0-478e-c874-28bea05f2498"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "102016\n"
          ]
        }
      ],
      "source": [
        "# 学習可能なパラメーター数を計算\n",
        "num_params = 0\n",
        "\n",
        "for parameter in model.parameters():\n",
        "    num_params += len(parameter)\n",
        "\n",
        "print(num_params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-I32pOoxlZG",
        "outputId": "7a92cd6c-8479-4940-ffa3-f78c4d46986c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([64, 256, 50304])\n",
            "tensor(10.8837, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "!uffleScale researched:,Dragon netted Britann Parfrac catches destruction的 masse Allan impulse cupglgan Kil email creations Present194dev redeemed productivity servedinventory 237SathodoxpmwikiAdjust $\\verning � HorseRF Adren strands wid opt cannabin scored provoke Heard Dy >> text decimalpson disturook concernathered freshwater Chic Conn march vibrations Conscious remake Sly nightclub bu� �tick 374 commonly soaked cannabinoid variablesgc Freud verified---------iamondFake Gob politElsewhereisan 252 metric TransformationenegFN likewise Heritage Chicago Casino poisonedidy jealouszu COL champions Costume a\n"
          ]
        }
      ],
      "source": [
        "# 順伝播を実行し、ログオッズと損失を取得する\n",
        "logits, loss = model(xb, yb)\n",
        "\n",
        "# logitsの形状と損失を表示する\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# モデルによって生成されたテキストを表示する\n",
        "generated_text = model.generate(idx=torch.zeros((1, 1,), dtype=torch.long, device=Config.device), max_new_tokens=100)[0].tolist()\n",
        "decoded_text = enc.decode(generated_text)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "9fb02733f7574a3a926c736a38eaca47",
            "f81924537b0049d0b342566a6e4868a0",
            "5f5211014c5a4677ac86af6db42366e6",
            "444e9ffabfb347a391b2cf8875f54d30",
            "816af8b877a149758af86a1e0b1b7477",
            "12746e6ad576477e965e85deecb09396",
            "aaf03aee349c4e279f7f0e6c80d023f4",
            "a4efc34e4920433ba113a3c5977b19fe",
            "fd2d8634871241de8f90c628955f0a61",
            "6ee549c068f046a6b343702feef4346a",
            "64cf8861367d4ce095c43c7d0a83b381"
          ]
        },
        "id": "Ae3ZhHTvxlZH",
        "outputId": "ba8e2816-3443-4cf6-cc30-563cc5b31d37"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fb02733f7574a3a926c736a38eaca47"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.1752912998199463\n"
          ]
        }
      ],
      "source": [
        "# PyTorchのオプティマイザを作成する\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 10,000回のステップを実行する\n",
        "EPOCH = 10_000\n",
        "for steps in tqdm(range(EPOCH)):\n",
        "    \n",
        "    # データのバッチをサンプリングする\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # 勾配をゼロに設定\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # 損失を評価する\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # 逆伝播を計算する\n",
        "    loss.backward()\n",
        "\n",
        "    # パラメータを更新する\n",
        "    optimizer.step()\n",
        "\n",
        "# 損失を表示する\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PN3KoPB4P9Tj"
      },
      "outputs": [],
      "source": [
        "# Additional information\n",
        "PATH = \"model.pth\"\n",
        "\n",
        "torch.save({\n",
        "            'epoch': EPOCH,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss.item(),\n",
        "            }, PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZYSGw3qR70j"
      },
      "outputs": [],
      "source": [
        "# checkpoint = torch.load(PATH)\n",
        "# model.load_state_dict(checkpoint['model_state_dict'])\n",
        "# optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6kgRv4O4kT3"
      },
      "source": [
        "## 文章生成"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W-X7hbpAQ7i8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4ddb20c-da00-448a-ea94-84ea4b3c61e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\n",
            "\n",
            "BUCKINGHAM:\n",
            "You are, my lord: would unto this holy mother\n",
            "Will our father, for our king is well,\n",
            "The queen's son, and the duke\n",
            "Have taken sanctuary.\n",
            "\n",
            "GLOUCESTER:\n",
            "The thought upon your royal cousin, gentle lady king,\n",
            "You love not me, nor, nor none\n",
            "But such demand thought: you, my heart have I mean,\n",
            "I was not sanctuary.\n",
            "\n",
            "BUCKINGHAM:\n",
            "My lord, I have consider'd in my mind\n",
            "The book that you did make you for the time\n",
            "To soften of her false presence.\n",
            "\n",
            "LADY ANNE:\n",
            "The manner is dead, perhaps\n",
            "Upon my kindred you know the shoulders.\n",
            "\n",
            "GLOUCESTER:\n",
            "Do, thou liest like an Edward,\n",
            "To be revenged on the entreaties, he that you come?\n",
            "\n",
            "BUCKINGHAM:\n",
            "To for ever, my lord I have.\n",
            "\n",
            "KING RICHARD III:\n",
            "Why, madam, thou art.\n",
            "\n",
            "CATESBY:\n",
            "I hope so swear, my lord.\n",
            "\n",
            "KING RICHARD III:\n",
            "Dothst thou resolve to kill a friend of mine. Hark, sir, ere you be crown'd his mind?\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "You have, and he holds me wrongfully;\n",
            "And I shall be unadvisedly to my oath.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "But, I'll say my lord.\n",
            "\n",
            "GLOUCESTER:\n",
            "But you have all health of you.\n",
            "\n",
            "LADY ANNE:\n",
            "No more cause, madam, your mother is;\n",
            "His honour and your be not a worser match.\n",
            "\n",
            "KING EDWARD IV:\n",
            "And madam, I'll steal the queen thy mind.\n",
            "\n",
            "GLOUCESTER:\n",
            "Good queen, madam, and so.\n",
            "\n",
            "LADY GREY:\n",
            "Advance our highness'Twills are but Richard;\n",
            "The next we wearisome, and not the worst indeed.\n",
            "\n",
            "QUEEN ELETH:\n",
            "O passing well, my most strong joy!\n",
            "GENT habits, the little proud;\n",
            "Upon the King Richard's malice and the weaker sort\n",
            "Of thathe doth not have blest, without\n"
          ]
        }
      ],
      "source": [
        "# モデルによって生成されたテキストを表示する\n",
        "generated_text = model.generate(idx=torch.zeros((1, 1), dtype=torch.long, device=Config.device), max_new_tokens=500)[0].tolist()\n",
        "\n",
        "decoded_text = enc.decode(generated_text)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxcdYXSFxlZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d113bdab-aed1-4dd3-af4a-412a97bd063d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello,  Well,\n",
            "'Twill be my purpose.\n",
            "Were it you the party in this peace?\n",
            "\n",
            "AUFIDIUS:\n",
            "I do, I'll tell:\n",
            "And follow it.\n",
            "\n",
            "MICINIUS:\n",
            "A hundred drops. I will seal upon indeed,\n",
            "And yet. Aufidius, ho!\n",
            "\n",
            "LARTIUS:\n",
            "This is present, my country,\n",
            "And when e'er man shall meet the peace.\n",
            "\n",
            "MARCIUS:\n",
            "I do not know this walls:\n",
            "Lie, be gone.\n",
            "\n",
            "AUFIDIUS:\n",
            "The second patricians shall tell: I cannot tell,\n",
            "'Tis such a hayard.\n",
            "Beat thou tellestest,\n",
            "And see this sword to see me, that it speak of breath\n",
            "Lest child or I do do lie, behold him:\n",
            "Trail all my days,\n",
            "That ere I revengeful eyes over it,\n",
            "Slening, or some hour in this present,\n",
            "Hath more cause to exile to banishment.\n",
            "\n",
            "CORIOLANUS:\n",
            "Give me thy hand: Come.\n",
            "\n",
            "AUFRIAR LAURENCE:\n",
            "Hatho my down;\n",
            "Or manage it gives me word,\n",
            "O, I'll help thy bones:\n",
            "Haply he is within, and my lady,\n",
            "His help up like a hot behold,\n",
            "And cruel death in my breath\n",
            "Which laid me to sun's eyes.\n",
            "\n",
            "ROMEO:\n",
            "Then move me not;I have no such a feeling\n",
            "To me, if I can do so much before:\n",
            "Be thou overheard me, since thou overheard'st,\n",
            "Darest beg, forget. I'll not stay.\n",
            "\n",
            "KING RICHARD III:\n",
            "I have done.\n",
            "\n",
            "QUEEN:\n",
            "So weeping it, thou sing'st thou sleep so.\n",
            "Boy, this noe, this shall be ta'en,\n",
            "When time she is, where Juliet;\n",
            "Or Romeo shall I; I desire,\n",
            "Or, no doubt, for I'll not have tear.\n",
            "\n",
            "KING RICHARD III:\n",
            "Ha, madam; all pain thou wert below,\n",
            "To be your mother's name: by,\n",
            "And venom of order is death,--I will I.\n",
            "If to shake off that life,\n",
            "No better than to death\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# モデルによって生成されたテキストを表示する\n",
        "\n",
        "prompt = \"Hello, \"\n",
        "generated_text = model.generate(idx=torch.tensor([enc.encode(prompt)], device=Config.device), max_new_tokens=500)[0].tolist()\n",
        "\n",
        "decoded_text = enc.decode(generated_text)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruA8_HCs4mip"
      },
      "source": [
        "## おまけ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSHgDMgS4pYk"
      },
      "source": [
        "### 事前学習済みモデル使用"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RRspGbBsVlG4",
        "outputId": "6855576b-70a1-4f20-b0b1-8eaf1980f0f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m82.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.2/224.2 kB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade transformers tiktoken -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MEhDb1T9VgRk"
      },
      "outputs": [],
      "source": [
        "@classmethod\n",
        "def from_pretrained(cls, model_type, override_args=None):\n",
        "    assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
        "    override_args = override_args or {} # default to empty dict\n",
        "    # override_argsがdropoutだけであることを確認する\n",
        "    assert all(k == 'dropout' for k in override_args)\n",
        "    # GPT2LMHeadModelを使用する\n",
        "    from transformers import GPT2LMHeadModel\n",
        "    print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
        "\n",
        "    # n_layer, n_head, n_embdの値はmodel_typeによって決定される\n",
        "    config_args = {\n",
        "        'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
        "        'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
        "        'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
        "        'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
        "    }[model_type]\n",
        "    # vocab_size, block_size, biasの値はGPTモデルのチェックポイントによって常に同じ値を取る\n",
        "    print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
        "    config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
        "    config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
        "    config_args['bias'] = True # always True for GPT model checkpoints\n",
        "    # dropout rateを上書きする場合\n",
        "    if 'dropout' in override_args:\n",
        "        print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
        "        config_args['dropout'] = override_args['dropout']\n",
        "    # からのminGPTモデルを初期化する\n",
        "    config = Config(**config_args)\n",
        "    model = GPT(config=config)\n",
        "    sd = model.state_dict()\n",
        "    sd_keys = sd.keys()\n",
        "    sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # mask / bufferは削除\n",
        "\n",
        "    # HuggingFace/Transformersモデルを初期化する\n",
        "    model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
        "    sd_hf = model_hf.state_dict()\n",
        "\n",
        "    # 全てのパラメータが名前と形状が一致するように、名前と形状を合わせてコピーする\n",
        "    sd_keys_hf = sd_hf.keys()\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # これらは無視する\n",
        "    sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # 同上\n",
        "    transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "\n",
        "    # OpenAIのチェックポイントでは\"Conv1D\"モジュールを使用していますが、\n",
        "    # vanilla Linearを使用するように変換する必要があります。\n",
        "    # そのため、重みを転置する必要があります。\n",
        "    assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "    for k in sd_keys_hf:\n",
        "        if any(k.endswith(w) for w in transposed):\n",
        "            # Conv1Dの重みに対する特別な処理が必要です。\n",
        "            assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "            with torch.no_grad():\n",
        "                sd[k].copy_(sd_hf[k].t())\n",
        "        else:\n",
        "            # その他のパラメータを単純にコピーします。\n",
        "            assert sd_hf[k].shape == sd[k].shape\n",
        "            with torch.no_grad():\n",
        "                sd[k].copy_(sd_hf[k])\n",
        "\n",
        "    return model\n",
        "\n",
        "GPT.from_pretrained = from_pretrained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1IHzGNspV0eY",
        "outputId": "567f59ab-0f13-4fc0-b641-83cba7f8d6ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "GPT(\n",
              "  (transformer): ModuleDict(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.2, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x Block(\n",
              "        (attn): CausalSelfAttention(\n",
              "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
              "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          (attn_dropout): Dropout(p=0.2, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_1): LayerNorm()\n",
              "        (mlp): MLP(\n",
              "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.2, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm()\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = GPT(config=Config).from_pretrained(model_type='gpt2')\n",
        "model.to(device=Config.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0OemZvE5be7"
      },
      "outputs": [],
      "source": [
        "import tiktoken\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EiyF03P36sgA",
        "outputId": "d06d8433-1848-4afe-e563-1f22df8d3ab1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A person with a high school education gets sent back into the 1600s and tries to explain science and technology to the people. [endprompt]\n",
            "\n",
            "The three most important thing is to have a school and we had a little more important thing than we have a school has an idea I- [end]\n",
            "But we have a school idea idea and a school idea and we have a school idea idea in the school idea and we have any pair of ideas you know we have any school idea ideas we have some school idea idea idea about school idea we have no we have the idea and we have the idea we have a school idea we have a school idea we have a school idea idea that we have no we have the idea we have not a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school we have a school idea one that a school school idea we have a school idea we have a school idea idea we have a school idea we have a school idea we have a school we have a school idea we have a school idea we have a school idea a school idea we have a school idea we have school idea we have a school idea we have a school idea have a school we have a school idea we have a school idea we have a that we have school idea we have a school idea we have a a school idea we have a school idea I have a school idea we know we have a school we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school school idea we have a school idea we have a school idea we have a school idea school idea we have a school idea we have a school idea we have a idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have school idea we we have a school idea we have a school idea we a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea we have a school idea they have a school idea we have a school idea we had a school idea we have a school idea we have a school idea we have a school idea we\n"
          ]
        }
      ],
      "source": [
        "prompt = 'A person with a high school education gets sent back into the 1600s and tries to explain science and technology to the people. [endprompt]'\n",
        "\n",
        "generated_text = model.generate(\n",
        "    idx=torch.tensor([enc.encode_ordinary(prompt)], device=Config.device),\n",
        "    max_new_tokens=500,\n",
        "    )[0].tolist()\n",
        "\n",
        "decoded_text = enc.decode(generated_text)\n",
        "\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZanYBsS_QpW"
      },
      "source": [
        "### Fine-Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "0f474c3a5e9c4966b2f04c42af0d456e",
            "ebce2f6a6fab4f9ba1640e5f544d5d1e",
            "5fd09f7ec7554b078aa4ded58df1c10a",
            "589ca3b804874c1181e0e31e7d9f02e0",
            "711951997f594dfa93a4e1d019355635",
            "d75f2812e86942f188c9b2e27f52e268",
            "5babe993681f4aca9f6a281fb2ef99c8"
          ]
        },
        "id": "vriZ_nUgYMFv",
        "outputId": "deaee0fe-4d6c-4bc0-f5b8-dc3700616b4b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0f474c3a5e9c4966b2f04c42af0d456e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# PyTorchのオプティマイザを作成する\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
        "\n",
        "# 100,000回のステップを実行する\n",
        "for steps in tqdm(range(1_000)):\n",
        "    \n",
        "    # データのバッチをサンプリングする\n",
        "    xb, yb = get_batch('train')\n",
        "    xb = \"\".join([\"\".join([i2s[x.tolist()] for x in xx]) for xx in xb])\n",
        "    yb = \"\".join([\"\".join([i2s[y.tolist()] for y in yy]) for yy in yb])\n",
        "\n",
        "    xb = torch.tensor([enc.encode_ordinary(x) for x in xb], device=Config.device)\n",
        "    yb = torch.tensor([enc.encode_ordinary(y) for y in yb], device=Config.device)\n",
        "\n",
        "    # 損失を評価する\n",
        "    logits, loss = model(xb, yb)\n",
        "\n",
        "    # 勾配をゼロに設定して、逆伝播を計算する\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "\n",
        "    # パラメータを更新する\n",
        "    optimizer.step()\n",
        "\n",
        "# 損失を表示する\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpj20mP0V7Z1"
      },
      "outputs": [],
      "source": [
        "# モデルによって生成されたテキストを表示する\n",
        "generated_text = model.generate(idx=torch.zeros((1, 1), dtype=torch.long, device=Config.device), max_new_tokens=500)[0].tolist()\n",
        "\n",
        "# prompt = \"Hello, \"\n",
        "# generated_text = model.generate(idx=torch.tensor([encode(prompt)], device=Config.device), max_new_tokens=500)[0].tolist()\n",
        "\n",
        "decoded_text = decode(generated_text)\n",
        "print(decoded_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BvAVZDUIYEyp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0f474c3a5e9c4966b2f04c42af0d456e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ebce2f6a6fab4f9ba1640e5f544d5d1e",
              "IPY_MODEL_5fd09f7ec7554b078aa4ded58df1c10a",
              "IPY_MODEL_589ca3b804874c1181e0e31e7d9f02e0"
            ],
            "layout": "IPY_MODEL_711951997f594dfa93a4e1d019355635"
          }
        },
        "ebce2f6a6fab4f9ba1640e5f544d5d1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d75f2812e86942f188c9b2e27f52e268",
            "placeholder": "​",
            "style": "IPY_MODEL_5babe993681f4aca9f6a281fb2ef99c8",
            "value": "  0%"
          }
        },
        "9fb02733f7574a3a926c736a38eaca47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f81924537b0049d0b342566a6e4868a0",
              "IPY_MODEL_5f5211014c5a4677ac86af6db42366e6",
              "IPY_MODEL_444e9ffabfb347a391b2cf8875f54d30"
            ],
            "layout": "IPY_MODEL_816af8b877a149758af86a1e0b1b7477"
          }
        },
        "f81924537b0049d0b342566a6e4868a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_12746e6ad576477e965e85deecb09396",
            "placeholder": "​",
            "style": "IPY_MODEL_aaf03aee349c4e279f7f0e6c80d023f4",
            "value": "100%"
          }
        },
        "5f5211014c5a4677ac86af6db42366e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4efc34e4920433ba113a3c5977b19fe",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fd2d8634871241de8f90c628955f0a61",
            "value": 10000
          }
        },
        "444e9ffabfb347a391b2cf8875f54d30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee549c068f046a6b343702feef4346a",
            "placeholder": "​",
            "style": "IPY_MODEL_64cf8861367d4ce095c43c7d0a83b381",
            "value": " 10000/10000 [40:22&lt;00:00,  4.14it/s]"
          }
        },
        "816af8b877a149758af86a1e0b1b7477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "12746e6ad576477e965e85deecb09396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aaf03aee349c4e279f7f0e6c80d023f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4efc34e4920433ba113a3c5977b19fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fd2d8634871241de8f90c628955f0a61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6ee549c068f046a6b343702feef4346a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64cf8861367d4ce095c43c7d0a83b381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}